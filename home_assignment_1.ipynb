{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c295abce",
   "metadata": {},
   "source": [
    "Text Analytics I HWS 22/23\n",
    "\n",
    "# Home Assignment 1 (35 pts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c27e5d",
   "metadata": {},
   "source": [
    "Submit your solution via Ilias until 23.59pm on Tuesday, October 4th. Late submissions are accepted until 10:15am on the following day (start of the exercise), with 1/4 of the total possible points deducted from the score.\n",
    "\n",
    "Submit your solutions in teams of 3-4 students. Unless explicitly agreed otherwise in advance, **submissions from teams with more or less members will NOT be graded**.\n",
    "List all members of the team with their student ID and full name in the cell below, and submit only one notebook per team. Only submit a notebook, do not submit the dataset(s) you used or image files that you have created - these have to be created from your notebook. Also, do NOT compress/zip your submission!\n",
    "\n",
    "Cite ALL your sources for coding this home assignment. In case of plagiarism (copying solutions from other teams or from the internet) ALL team members will be expelled from the course without warning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b570d3",
   "metadata": {},
   "source": [
    "#### General guidelines:\n",
    "* Make sure that your code is executable, any task for which the code does not directly run on our machine will be graded with 0 points.\n",
    "* Use only packages that are automatically installed along with Anaconda, plus some additional packages that have been introduced in the context of this class.\n",
    "* Ensure that the notebook does not rely on the current notebook or system state!\n",
    "  * Use `Kernel --> Restart & Run All` to see if you are using any definitions, variables etc. that \n",
    "    are not in scope anymore.\n",
    "  * Do not rename any of the datasets you use, and load it from the same directory that your ipynb-notebook is located in, i.e., your working directory.\n",
    "* Make sure you clean up your code before submission, e.g., properly align your code, and delete every line of code that you do not need anymore, even if you may have experimented with it. Minimize usage of global variables. Do not reuse variable names multiple times!\n",
    "* Ensure your code/notebook terminates in reasonable time.\n",
    "* Feel free to use comments in the code. While we do not require them to get full marks, they may help us in case your code has minor errors.\n",
    "* For questions that require a textual answer, please do not write the answer as a comment in a code cell, but in a Markdown cell below the code. Always remember to provide sufficient justification for all answers.\n",
    "* You may create as many additional cells as you want, just make sure that the solutions to the individual tasks can be found near the corresponding assignment.\n",
    "* If you have any general question regarding the understanding of some task, do not hesitate to post in the student forum in Ilias, so we can clear up such questions for all students in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "04d29726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# credentials of all team members\n",
    "team_members = [\n",
    "    {\n",
    "        'first_name': 'Pavani',\n",
    "        'last_name': 'Rajula',\n",
    "        'student_id': 1870208\n",
    "    },\n",
    "    {\n",
    "        'first_name': 'Yves',\n",
    "        'last_name': 'Staudenmaier',\n",
    "        'student_id': 1754294\n",
    "    },\n",
    "    {\n",
    "        'first_name': 'Zeynep',\n",
    "        'last_name': 'Eroglu',\n",
    "        'student_id': 1834472\n",
    "    },\n",
    "    {\n",
    "        'first_name': 'Priyanka',\n",
    "        'last_name': 'Roy',\n",
    "        'student_id': 1933097\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c2c397d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union, Dict, Set, Tuple\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b9f30f",
   "metadata": {},
   "source": [
    "### Task 1: Regular expressions (9 pts)\n",
    "In this task you are asked to create regular expressions that meet the specified conditions. Please use the ``re`` package for the following subtasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20076392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the re package \n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83245d4",
   "metadata": {},
   "source": [
    "__a)__ Write a regular expression that returns all integer numbers from a text that are surrounded by whitespaces. __(1 pt)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1623b9a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0000', '1234', '2029', '12', '1200', '456', '3245', '2019']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "text = ' 0000 test 1234 hey h3110 2029 w0r1d 12 1234 1200 123, 456 0002 hello 3245 12345 2019 end.'\n",
    "re.findall(r' (\\d+) ', text)\n",
    "# re.findall(r'\\b\\d+\\b', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268f05db",
   "metadata": {},
   "source": [
    "__b)__ Write a regular expression that returns all valid years that are surrounded by whitespaces in a text. A valid year is a 4 digit number in the range from 0000 to 2022. __(2 pts)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d97f7592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0000', '1234', '1234', '0002', '2019']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "re.findall(r' ([0-1][0-9]{3}|20[0-1][0-9]|202[0-2]) ', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea7b9ad",
   "metadata": {},
   "source": [
    "__c)__ Write a regular expression that returns all dates in the format YYYY-MM-DD or YYYY/MM/DD from a given text. Make sure that YYYY is a valid year (see task __b)__), MM is a valid month (1 to 12) and DD is a valid day (1 to 31).\n",
    "There is no need to make sure that e.g. XXXX-02-31 does not exist.  __(2 pts)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fecd3a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2022', '09', '31'), ('2022', '08', '31'), ('1998', '02', '31')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "text = '2022-09-31 2022-08-31 does not exist and also 1998/02/31'\n",
    "re.findall(r'([01][0-9]{3}|20[0-1][0-9]|202[0-2])[- /](0[1-9]|1[1-2])[- /]([0-2][0-9]|3[0-1])', text)\n",
    "# re.findall(r'\\b\\d{4}-\\d\\d?-\\d\\d?\\b', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a035a263",
   "metadata": {},
   "source": [
    "__d)__ Assume you are given a list ``l`` of strings like the one below. Using regular expressions, return a list that contains all elements from ``l`` that don't contain both, the letter ``a`` and ``e`` and store the result in a variable ``l_filtered``.  __(2 pts)__\n",
    "\n",
    "__Example:__ _given the list_  \n",
    "\n",
    "``l = [\"apple\", \"cucumber\", \"tomato\", \"zucchini\", \"pumpkin\", \"pear\", \"raspberry\", \"blueberry\"]``  \n",
    "\n",
    "_you should return_  \n",
    "\n",
    "``l_filtered = ['cucumber', 'tomato', 'zucchini', 'pumpkin', 'blueberry']``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "47106934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example list\n",
    "l = [\"apple\", \"cucumber\", \"tomato\", \"zucchini\", \"pumpkin\", \"pear\", \"raspberry\", \"blueberry\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d1325041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cucumber', 'tomato', 'zucchini', 'pumpkin', 'blueberry']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "l_filtered = [word for word in l if not re.match(r'(?=.*a)(?=.*e).*$', word)]\n",
    "l_filtered "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b905ecc7",
   "metadata": {},
   "source": [
    "__e)__ For the given string ``s`` with 4 lines, change the _whole_ word ``pot`` (i.e. ``pottery`` should not be changed) to ``1234`` only if it is at the start of a line.  __(2 pts)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "966e2b00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pottery clot pot \\npot dot plot hot\\nspot rot pot got\\nnot shot forgot'"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = '''\\\n",
    "pottery clot pot \n",
    "pot dot plot hot\n",
    "spot rot pot got\n",
    "not shot forgot'''\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "50f05f6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pottery clot pot \\n1234 dot plot hot\\nspot rot pot got\\nnot shot forgot'"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "s = '\\n'.join([re.compile(r'^pot ').sub('1234 ', sent, 1) for sent in s.split('\\n') ])\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8e4d62",
   "metadata": {},
   "source": [
    "### Task 2: Finding the most similar word (18 pts)\n",
    "The goal of this task is, given a corpus, to find the most similar word for a provided word. As an example we will consider the King James Bible that is is included in the ``gutenberg`` corpus and we are looking to find the word that is most similar to ``god``. We consider two words similar if they appear in the same word context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4036fc75",
   "metadata": {},
   "source": [
    "__a) Cleaning the input (3 pts)__\n",
    "\n",
    "Write a function that, given a list of tokens, returns a list of tokens that we consider valid for our task. \n",
    "\n",
    "An *invalid* token is a token that \n",
    "- is a punctuation mark (consider `string.punctuation`).\n",
    "- is entirely numeric digits (e.g. `\"123\"`)\n",
    "- optionally, if `remove_stopwords=True` then stopwords in the English language are also not considered valid tokens (use nltk's stopwords)\n",
    "\n",
    "Use the function signature specified in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "975b8007",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pavanirajula/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from curses.ascii import isdigit\n",
    "import nltk\n",
    "from nltk.corpus.reader.util import StreamBackedCorpusView \n",
    "import string \n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c120c127",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_tokens(tokens: Union[List[str], StreamBackedCorpusView], remove_stopwords: bool=False) -> List[str]:\n",
    "    \"\"\"\n",
    "    :param tokens: list of tokens that should be cleaned\n",
    "    :param remove_stopwords: bool indicating if stopwords should be removed \n",
    "                             False by default\n",
    "    :return: list of valid tokens\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    invalid_tokens = list(string.punctuation) + stopwords if remove_stopwords == True else []\n",
    "    return [token.lower() for token in tokens if not token.isdigit() and token.lower() not in invalid_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "c2cf9807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download(\"gutenberg\")\n",
    "# from nltk.corpus import gutenberg\n",
    "# tokens = nltk.corpus.gutenberg.words('bible-kjv.txt')\n",
    "\n",
    "# clean_words = get_valid_tokens(tokens,True)\n",
    "# print(sorted(clean_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81184587",
   "metadata": {},
   "source": [
    "__b) Counting the surroundings (6 pts)__\n",
    "\n",
    "In our simple model of word similarity we consider words as similar if they are being used in the same context (i.e. they have similar words surrounding them). \n",
    "\n",
    "Implement a function that, given a list of words, returns the count of all words in a designated word's surrounding. The context size indicates how many words to the left and right we consider, i.e. a context size of 1 indicates that we only consider the words immediately before and after a central word to be in its context. \n",
    "\n",
    "Your function should return a dictionary which maps each word $w$ from the input list to a dictionary. The inner dictionary should map each word that appears in the context of the central word $w$ to a number that indicates how often it appears in the context of $w$.\n",
    "\n",
    "Make sure your implementation has linear complexity in the vocabulary / corpus length. Use the function signature specified in the cell below.\n",
    "\n",
    "__Hint:__ Instead of the inner dictionary you can alternatively use `Counter` or `defaultdict` which can be found in the Python module `collections`. Moreover, consider the function ``ngrams`` from ``nltk``.\n",
    "\n",
    "__Example:__ _For the input_\n",
    "`['hi', 'james', 'how', 'are', 'you', 'hello', 'catherine', 'how', 'are', 'you']` _and_ `context_size=1`\n",
    "_we wish to obtain:_\n",
    "```\n",
    "{'hi': {'james': 1},\n",
    " 'james': {'hi': 1, 'how': 1},\n",
    " 'how': {'james': 1, 'are': 2, 'catherine': 1},\n",
    " 'are': {'how': 2, 'you': 2},\n",
    " 'you': {'are': 2, 'hello': 1},\n",
    " 'hello': {'you': 1, 'catherine': 1},\n",
    " 'catherine': {'hello': 1, 'how': 1}}\n",
    "```\n",
    "__Explanation:__ _The word_ `hi` _is only surrounded by_ `james`_. The word_ `james` _is surrounded by_ `hi` _and_ `how` _. The word_ `how` _is surrounded by_ `james`_, by_ `catherine` _and by_ `are` _twice, ..._\n",
    "\n",
    "_For_ `contextsize=2` _we obtain:_\n",
    "```\n",
    "{'hi': Counter({'james': 1, 'how': 1}),\n",
    "'james': Counter({'hi': 1, 'how': 1, 'are': 1}),\n",
    "'how': Counter({'are': 2, 'you': 2, 'hi': 1, 'james': 1, 'hello': 1, 'catherine': 1}),\n",
    "'are': Counter({'how': 2, 'you': 2, 'james': 1, 'hello': 1, 'catherine': 1}),\n",
    "'you': Counter({'how': 2, 'are': 2, 'hello': 1, 'catherine': 1}),\n",
    "'hello': Counter({'are': 1, 'you': 1, 'catherine': 1, 'how': 1}),\n",
    "'catherine': Counter({'you': 1, 'hello': 1, 'how': 1, 'are': 1})}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a911633",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Counter\n",
    "\n",
    "\n",
    "def get_surrounding_counts(tokens: Union[List[str], StreamBackedCorpusView], context_size: int) -> Dict[str, Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    :param tokens: list of tokens\n",
    "    :param context_size: integer that indicates the number of context words that are considered on both sides of the central word\n",
    "    :return: dict of dicts that holds the count of context words for each input token\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    surrounding_word_counter = {}\n",
    "    for token in set(tokens):\n",
    "        word_counter = Counter()\n",
    "        indices = [i for i,word in enumerate(tokens) if word == token]\n",
    "        for index in indices:\n",
    "            for word in tokens[:index][-context_size:] + tokens[index+1:context_size+index+1]: \n",
    "                word_counter[word] += 1\n",
    "        surrounding_word_counter[token] = word_counter\n",
    "    return surrounding_word_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88b68988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'are': Counter({'how': 2, 'you': 2, 'james': 1, 'hello': 1, 'catherine': 1}), 'hello': Counter({'are': 1, 'you': 1, 'catherine': 1, 'how': 1}), 'how': Counter({'are': 2, 'you': 2, 'hi': 1, 'james': 1, 'hello': 1, 'catherine': 1}), 'catherine': Counter({'you': 1, 'hello': 1, 'how': 1, 'are': 1}), 'james': Counter({'hi': 1, 'how': 1, 'are': 1}), 'hi': Counter({'james': 1, 'how': 1}), 'you': Counter({'how': 2, 'are': 2, 'hello': 1, 'catherine': 1})}\n"
     ]
    }
   ],
   "source": [
    "surround_words_counter = get_surrounding_counts(['hi', 'james', 'how', 'are', 'you', 'hello', 'catherine', 'how', 'are', 'you'],2)\n",
    "# surround_words_counter = get_surrounding_counts(clean_words, 2)\n",
    "print(surround_words_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841e226d",
   "metadata": {},
   "source": [
    "__c) Keeping the top $k$ words in context (2 pts)__\n",
    "\n",
    "To reduce the size of our result from task __b)__, we will only consider the most frequent context words for each token. Therefore, write a function that keeps only the $k$ most frequent words in the context of a designated word. Ties are resolved in lexicographic order (e.g. _**A**braham_ would be preferred to _**B**ethlehem_). The function should return a dictionary that maps each word in the outer dictionary to a __set__ of their top $k$ most frequent context words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "106f0d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sets(context_dict: Dict[str, Dict[str, int]], k: int) -> Dict[str, Set[str]]:\n",
    "    \"\"\"\n",
    "    :param context_dict: dict of dicts that holds the count of context words for each word\n",
    "    :param k: integer that specifies how many context words should be kept\n",
    "    :return: dict that maps each word to a set of its k most frequent context words\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    for token in context_dict.keys():\n",
    "        context_dict[token] = set([word[0] for word in context_dict[token].most_common(k)])\n",
    "    return context_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c3b8970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'are': {'how'}, 'hello': {'are'}, 'how': {'are'}, 'catherine': {'you'}, 'james': {'hi'}, 'hi': {'james'}, 'you': {'how'}}\n"
     ]
    }
   ],
   "source": [
    "topk_surround_words = to_sets(surround_words_counter, 1)\n",
    "print(topk_surround_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc84aa14",
   "metadata": {},
   "source": [
    "__d) Finding the most similar words (4 pts)__\n",
    "\n",
    "Given a dictionary as obtained in task __c)__ and a word $w$, we want to find the words that have the highest similarity to $w$ in terms of their context. We operationalize context similarity with the Jaccard index (https://en.wikipedia.org/wiki/Jaccard_index).\n",
    "The Jaccard index of two sets $A$ and $B$ (in our case the sets of context words) is defined as:\n",
    "\n",
    "$J(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}$\n",
    "\n",
    "Write a function that returns the words that have the highest similarity to an input word $w$ (ignoring the input word itself). Since several words may have the same Jaccard similarity to the input word, your function should return the set of words that are most similar to $w$ and the respective value of the Jaccard similarity. Use the function signature specified in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "860e680a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "def find_most_similar_words(input_word: str, contexts: Dict[str, Set[str]]) -> (Set[str], float):\n",
    "    \"\"\"\n",
    "    :param input_word: string that represents the word we are interested in\n",
    "    :param contexts: dict that maps each word to a set of its most frequent context words\n",
    "    :returns: \n",
    "        - set of the most similar words to the input word\n",
    "        - float that indicates the highest Jaccard similarity to the input word\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    input_word_context = [word for word in contexts[input_word] if word != input_word]\n",
    "    word_sim_scores = {}\n",
    "    for word in input_word_context:\n",
    "        set1 = set(input_word_context)\n",
    "        set2 = set([word for word in contexts[word] if word != input_word])\n",
    "        similarity = len(set(set1.intersection(set2)))/(len(set1.union(set2)))\n",
    "        word_sim_scores[word] = similarity\n",
    "    word_sim_scores = sorted(word_sim_scores.items(), key = lambda item:item[1], reverse=True)\n",
    "    return set([word[0] for word in word_sim_scores if math.ceil(word[1]) == math.ceil(word_sim_scores[0][1])]), word_sim_scores[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25858891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'hi'}, 0.0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_most_similar_words('james', topk_surround_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca0214f",
   "metadata": {},
   "source": [
    "__e) Bringing it all together (3 pts)__\n",
    "\n",
    "Finally, we want to apply our functions to the King James Bible (`'bible-kjv.txt'`) that is part of the ``gutenberg`` corpus. We intend to find the word that is most similar to ``god``. Follow the steps below:\n",
    "\n",
    "i) Obtain a list of all tokens from the King James Bible and store it in a variable ``tokens``.  \n",
    "\n",
    "ii) Clean the list of tokens with your function from __a)__ to get the list of valid tokens (without removing stopwords) and store it in a variable ``valid_tokens``.  \n",
    "\n",
    "iii) Apply your function from __b)__ to count the context words for all valid tokens with a ``context_size`` of 2 and store the result in a variable ``context_counts``.  \n",
    "\n",
    "iv) Using your function from __c)__, keep only the 20 most frequent words in a valid tokens context and store the result in a variable ``context_words``.  \n",
    "\n",
    "v) Finally, find the most similar words to the word _god_ with your function from __d)__ and store the set of most similar words in the variable ``set_god`` and the highest Jaccard similarity in the variable ``sim_god``.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8cfb84ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'called', 'said', 'blessed', 'kind', 'saw', 'work', 'good', 'lord', 'created', 'heaven', 'firmament', 'every', 'man', 'light', 'made', 'waters', 'upon', 'earth', 'let', 'day'} 0.39285714285714285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/pavanirajula/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "nltk.download(\"gutenberg\")\n",
    "from audioop import tostereo\n",
    "from nltk.corpus import gutenberg\n",
    "tokens = nltk.corpus.gutenberg.words('bible-kjv.txt')\n",
    "\n",
    "valid_tokens = get_valid_tokens(tokens, True)\n",
    "context_counts = get_surrounding_counts(valid_tokens,2)\n",
    "context_words = to_sets(context_counts, 20)\n",
    "set_god, sim_god = find_most_similar_words('god', context_words)\n",
    "print(set_god, sim_god)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c303e077",
   "metadata": {},
   "source": [
    "### Task 3: Minimum cost string alignment (8 pts)\n",
    "\n",
    "In this tak we want to compute an alignment between two strings, that has minimum edit distance. \n",
    "\n",
    "Implement a function that takes two strings and their edit distance matrix and returns a minimum cost alignment. You can assume that the edit distance matrix is provided by the function that you implemented in exercise 3, task 2, with a substitution cost of 2. \n",
    "\n",
    "A minimum cost alignment consists of two strings that, printed below each other comprise the alignment, where insertions and deletions are represented by a ``*``. Use the function signature in the cell below.\n",
    "\n",
    "__Example:__ _Given the input strings_ ``\"INTENTION\"`` _and_ ``\"EXECUTION\"`` _and the corresponding edit distance matrix:_\n",
    "\n",
    "``[[ 9  8  9 10 11 12 11 10  9  8]\n",
    " [ 8  7  8  9 10 11 10  9  8  9]\n",
    " [ 7  6  7  8  9 10  9  8  9 10]\n",
    " [ 6  5  6  7  8  9  8  9 10 11]\n",
    " [ 5  4  5  6  7  8  9 10 11 10]\n",
    " [ 4  3  4  5  6  7  8  9 10  9]\n",
    " [ 3  4  5  6  7  8  7  8  9  8]\n",
    " [ 2  3  4  5  6  7  8  7  8  7]\n",
    " [ 1  2  3  4  5  6  7  6  7  8]\n",
    " [ 0  1  2  3  4  5  6  7  8  9]]``,\n",
    "  \n",
    "_your function should return the two strings_ ``INTE***NTION`` _and_ ``***EXECUTION`` _that represent the alignment, when printed below each other:_\n",
    " \n",
    " ``INTE***NTION``    \n",
    " ``***EXECUTION`` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11310289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "def get_alignment(str1: str, str2: str, D: NDArray[NDArray[int]]) -> (str, str): \n",
    "    '''\n",
    "    :param str1: first string for alignment\n",
    "    :param str2: second string for alignment\n",
    "    :param D: edit distance matrix of str1 and str2\n",
    "    :returns: tuple of strings that indicate the alignment of the input strings\n",
    "    '''\n",
    "    # your code here\n",
    "    row, col = len(str1), len(str2)\n",
    "    str1, str2 =  list(str1), list(str2)\n",
    "    new_str1, new_str2 = [],[]\n",
    "\n",
    "    while True:\n",
    "#         print(D[row,col])\n",
    "        cost = 0 if str1[row-1] == str2[col-1] else 2   \n",
    "        if D[row,col] == D[row-1,col-1] + cost :\n",
    "            new_str1, new_str2 = [str1[row-1]] + new_str1, [str2[col-1]] + new_str2\n",
    "#             print(str1[row-1], str2[col-1])\n",
    "            row, col = row - 1, col - 1\n",
    "        else :\n",
    "            if D[row,col] == D[row-1,col] + 1:\n",
    "                new_str1, new_str2 = [str1[row-1]] + new_str1, ['*'] + new_str2\n",
    "                row, col = row - 1, col\n",
    "            elif D[row,col] == D[row,col-1] + 1:\n",
    "                new_str1, new_str2 = ['*'] + new_str1, [str2[col-1]] + new_str2\n",
    "                row, col = row, col - 1\n",
    "#         print(row, col, new_str1, new_str2)\n",
    "        if row == 0 and col == 0:\n",
    "            return ''.join(new_str1), ''.join(new_str2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8615a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2  3  4  5  6  7  8  9]\n",
      " [ 1  2  3  4  5  6  7  6  7  8]\n",
      " [ 2  3  4  5  6  7  8  7  8  7]\n",
      " [ 3  4  5  6  7  8  7  8  9  8]\n",
      " [ 4  3  4  5  6  7  8  9 10  9]\n",
      " [ 5  4  5  6  7  8  9 10 11 10]\n",
      " [ 6  5  6  7  8  9  8  9 10 11]\n",
      " [ 7  6  7  8  9 10  9  8  9 10]\n",
      " [ 8  7  8  9 10 11 10  9  8  9]\n",
      " [ 9  8  9 10 11 12 11 10  9  8]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('INTE*NTION', '*EXECUTION')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_edit_dist = [[ 9,  8,  9, 10, 11, 12, 11, 10,  9,  8],\n",
    " [ 8,  7,  8,  9, 10, 11, 10,  9,  8,  9],\n",
    " [ 7,  6,  7,  8,  9, 10,  9,  8,  9, 10],\n",
    " [ 6,  5,  6,  7,  8,  9,  8,  9, 10, 11],\n",
    " [ 5,  4,  5,  6,  7,  8,  9, 10, 11, 10],\n",
    " [ 4,  3,  4,  5,  6,  7,  8,  9, 10,  9],\n",
    " [ 3,  4,  5,  6,  7,  8,  7,  8,  9,  8],\n",
    " [ 2,  3,  4,  5,  6,  7,  8,  7,  8,  7],\n",
    " [ 1,  2,  3,  4,  5,  6,  7,  6,  7,  8],\n",
    " [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9]]\n",
    "min_edit_dist =np.rot90(np.transpose(np.asmatrix(min_edit_dist)))\n",
    "print(min_edit_dist)\n",
    "get_alignment(\"INTENTION\",\"EXECUTION\", min_edit_dist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6ca1c5bf6704cdd1db69b7e444f7a5c87e9c87b066cc2cc889d7c03db11d05f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
